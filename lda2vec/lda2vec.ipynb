{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 论文资料\n",
    "\n",
    "论文地址: [Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec](https://arxiv.org/abs/1605.02019)\n",
    "\n",
    "作者对论文的解读: https://multithreaded.stitchfix.com/blog/2016/05/27/lda2vec/\n",
    "\n",
    "作者代码地址: https://github.com/cemoody/lda2vec (python2, chianer)\n",
    "\n",
    "社区tensforflow实现代码地址: https://github.com/nateraw/Lda2vec-Tensorflow\n",
    "\n",
    "社区pytorch实现代码地址: https://github.com/TropComplique/lda2vec-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 需要解决的问题\n",
    "\n",
    "- 对于LDA来说，没有利用词语之间的语义信息，聚焦的是文档全局的信息。\n",
    "- 对于word2vec来说，没有考虑文档自身的主题，聚焦的是词语间局部的信息。\n",
    "\n",
    "作者博客有更详细的解释"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方法\n",
    "\n",
    "## 核心思想\n",
    "\n",
    "同时训练主题模型和词向量\n",
    "\n",
    "## 实现思路\n",
    "\n",
    "1. 首先看word2vec的似然函数(SGNS: skipgram negative-sampling):\n",
    "$$L = \\sum\\limits_{(i,j)} {[\\sigma (\\overrightarrow {{c_j}}  \\cdot \\overrightarrow {{w_i}} ) + \\sigma ( - \\overrightarrow {{c_j}}  \\cdot \\overrightarrow {{w_{negative}}} )]} $$\n",
    "该损失函数达到的目标是能够区分正负样本的词语对$(\\overrightarrow {{c_j}} ,\\overrightarrow {{w_i}} )$，其中context word就是中心词，即$\\overrightarrow {{c_j}}  = \\overrightarrow {{w_j}} $\n",
    "2. 为了引入文档全局的信息，lda2vec定义context word为$$\\overrightarrow {{c_j}}  = \\overrightarrow {{w_j}}  + \\overrightarrow {{d_j}} $$\n",
    "其中$\\overrightarrow {{d_j}} $为文档的向量，例如在某文档的主题为水果，苹果这个词代表水果，那么它更应该与橘子相似，但是另外一篇文档的主题为科技，苹果是指苹果公司，那么它更应该与微软相似。\n",
    "3. 类似于LDA的思想，定义文档向量为$$\\overrightarrow {{d_j}}  = {a_{j0}} \\cdot \\overrightarrow {{t_0}}  + {a_{j1}} \\cdot \\overrightarrow {{t_1}}  + ...$$\n",
    "其中$\\overrightarrow {{t_0}} ,\\overrightarrow {{t_1}} ,...\\overrightarrow {{t_k}}$为k个主题的向量，$0 \\le {a_{jk}} \\le 1$为对应权重。\n",
    "**注意，为保证${a_{jk}}$之和为1，实现中需要经过softmax转换，这样做的目的是让文档向量具有可解释性**\n",
    "4. 同时，为了使得主题权重稀疏，在word2vec的似然函数的基础上增加一项$$\\lambda \\sum\\limits_j {\\log q({d_j}|\\alpha )} $$\n",
    "其中$\\lambda$和$\\alpha$均为超参，$j$是指第$j$个文档$$q({d_j}|\\alpha ) = \\sum\\limits_k {(\\alpha  - 1)\\log {a_{jk}}} $$\n",
    "\n",
    "这一项在论文中称为Dirichlet似然，关于DIrichlet分布，参考http://blog.bogatron.net/blog/2014/02/02/visualizing-dirichlet-distributions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优点\n",
    "\n",
    "深度学习结果的可解释性：词向量、主题向量、文档向量纬度相同，均在同一个向量空间中。可以通过计算主题向量与词向量的距离，找出主题最相关的词语，来解释某个主题，然后根据文档向量在主题向量上的稀疏分布，找出文档的主题\n",
    "\n",
    "*作者的解释：不一定需要lda2vec，如果只需要主题解释，可以只用LDA，如果只需要词语的数字特征，可以只用word2vec。训练lda2vec比较耗费计算资源。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
