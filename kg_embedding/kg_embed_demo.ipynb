{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14951\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "fuzzy_name_2_entity_name = dict()\n",
    "with open(\"FB15k_mid2name.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        e_id, e_name = line.split()\n",
    "        fuzzy_name_2_entity_name[e_id] = e_name\n",
    "\n",
    "print(len(fuzzy_name_2_entity_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/m/0bjkk9', 'Nebraska_Cornhuskers_football')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(list(fuzzy_name_2_entity_name.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14951\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "id_2_entity_name = dict()\n",
    "with open(os.path.join(\"benchmarks\", \"FB15K\", \"entity2id.txt\"), \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i != 0:\n",
    "            e_name, e_id = line.split()\n",
    "            id_2_entity_name[int(e_id)] = fuzzy_name_2_entity_name[e_name]\n",
    "\n",
    "print(len(id_2_entity_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4756, 'Days_of_Being_Wild')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(list(id_2_entity_name.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1345\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "id_2_relation_name = dict()\n",
    "with open(os.path.join(\"benchmarks\", \"FB15K\", \"relation2id.txt\"), \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i != 0:\n",
    "            r_name, r_id = line.split()\n",
    "            id_2_relation_name[int(r_id)] = r_name\n",
    "\n",
    "print(len(id_2_relation_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(958, '/religion/religion/is_part_of')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(list(id_2_relation_name.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def string_format_triple(e1, e2, rel):\n",
    "    e1_name = id_2_entity_name[e1]\n",
    "    e2_name = id_2_entity_name[e2]\n",
    "    r_name = id_2_relation_name[rel]\n",
    "    return f\"{e1_name}, {r_name}, {e2_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483142\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "train_triples = []\n",
    "with open(os.path.join(\"benchmarks\", \"FB15K\", \"train2id.txt\"), \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i != 0:\n",
    "            e1, e2, r = line.split()\n",
    "            triple = (int(e1), int(e2), int(r))\n",
    "            train_triples.append(triple)\n",
    "\n",
    "print(len(train_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows = []\n",
    "for h,t,r in train_triples:\n",
    "    df_rows.append({\n",
    "        \"h\": str(h),\n",
    "        \"t\": str(t),\n",
    "        \"r\": str(r)\n",
    "    })\n",
    "triple_df = pd.DataFrame(df_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, #%%\n",
    "h_values, h_counts = np.unique(triple_df[\"h\"].value_counts().values, return_counts=True)\n",
    "# 出现i次(i个三元组)的不同头实体个数为appear_count_2_head_entity_count[i]，例如i为1，表明appear_count_2_head_entity_count[1]个实体只出现了一次\n",
    "appear_count_2_head_entity_count = dict(zip(h_values, h_counts))\n",
    "t_values, t_counts = np.unique(triple_df[\"t\"].value_counts().values, return_counts=True)\n",
    "appear_count_2_tail_entity_count = dict(zip(t_values, t_counts))\n",
    "r_values, r_counts = np.unique(triple_df[\"r\"].value_counts().values, return_counts=True)\n",
    "appear_count_2_relation_count = dict(zip(r_values, r_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({\"values\": values, \"counts\": counts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1289d0080>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1289cbb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df[\"counts\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3187, 3722, 140) Concord_Records, /music/record_label/artist, Sheila_E.\n"
     ]
    }
   ],
   "source": [
    "x = random.choice(train_triples)\n",
    "print(x, string_format_triple(*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "valid_triples = []\n",
    "with open(os.path.join(\"benchmarks\", \"FB15K\", \"valid2id.txt\"), \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i != 0:\n",
    "            e1, e2, r = line.split()\n",
    "            triple = (int(e1), int(e2), int(r))\n",
    "            valid_triples.append(triple)\n",
    "            \n",
    "print(len(valid_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(403, 6855, 91) Topic_Webpage, /common/annotation_category/annotations./common/webpage/topic, Henry_Rollins\n"
     ]
    }
   ],
   "source": [
    "x = random.choice(valid_triples)\n",
    "print(x, string_format_triple(*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59071\n"
     ]
    }
   ],
   "source": [
    "test_triples = []\n",
    "with open(os.path.join(\"benchmarks\", \"FB15K\", \"test2id.txt\"), \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i != 0:\n",
    "            e1, e2, r = line.split()\n",
    "            triple = (int(e1), int(e2), int(r))\n",
    "            test_triples.append(triple)\n",
    "test_count = len(test_triples)            \n",
    "print(test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8037, 846, 40) University_of_Westminster, /education/educational_institution/students_graduates./education/education/student, Richard_Wright\n"
     ]
    }
   ],
   "source": [
    "x = random.choice(test_triples)\n",
    "print(x, string_format_triple(*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_triples_train = set(train_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_triples_train_valid = set(train_triples + valid_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_triples_train_valid_test = set(train_triples + valid_triples + test_triples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import to neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from py2neo import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph(user=\"neo4j\", password=\"hitales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "清空图：\n",
    "MATCH (n) DETACH DELETE n\n",
    "\n",
    "不过滤查询：\n",
    "MATCH (n) RETURN n LIMIT 100\n",
    "\n",
    "新建node（如果不存在）：\n",
    "MERGE (e:Entity {name: 'Alice'}) \n",
    "ON CREATE SET e.name = \"Alice\"\n",
    "RETURN e;\n",
    "\n",
    "新建关系：\n",
    "MATCH (h:Entity), (t:Entity)\n",
    "WHERE h.name = 'Alice' AND t.name = 'Andy'\n",
    "CREATE (h)-[r:Relation {name: 'isFriends'}]->(t)\n",
    "\n",
    "找出有最多关系的尾实体:\n",
    "MATCH ()-[r]->(n)\n",
    "WITH n, count(r) as rel_cnt\n",
    "WHERE rel_cnt > 1\n",
    "RETURN n.name, rel_cnt ORDER BY rel_cnt DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_entity_on_graph(entity_id):\n",
    "    entity_name = id_2_entity_name[entity_id]\n",
    "    # \\是特殊字符\n",
    "    entity_name = entity_name.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "    entity_node_creation = f\"CREATE (e:Entity {{name: \\\"{entity_name}\\\"}})\"\n",
    "    g.run(entity_node_creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_relation_on_graph(h, t, r):\n",
    "    h_name = id_2_entity_name[h]\n",
    "    h_name = h_name.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "    t_name = id_2_entity_name[t]\n",
    "    t_name = t_name.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "    r_name = id_2_relation_name[r]\n",
    "    r_name = r_name.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "    relation_creation = f\"MATCH (h:Entity), (t:Entity) WHERE h.name = \\\"{h_name}\\\" AND t.name = \\\"{t_name}\\\" CREATE (h)-[r:Relation {{name: \\\"{r_name}\\\"}}]->(t);\"\n",
    "    g.run(relation_creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = random.choice(train_triples)\n",
    "print(x, string_format_triple(*x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create_entity_on_graph(1785)\n",
    "create_entity_on_graph(11751)\n",
    "create_relation_on_graph(1785, 11751, 105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14951/14951 [00:36<00:00, 410.64it/s]\n"
     ]
    }
   ],
   "source": [
    "for entity_id in tqdm(id_2_entity_name):\n",
    "    create_entity_on_graph(entity_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for triple in tqdm(train_triples):\n",
    "    create_relation_on_graph(*triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14951"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.evaluate(\"MATCH (e:Entity) RETURN count(e)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.run(\"MATCH (n) RETURN n LIMIT 10\").to_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "实体个数: 14951, 关系个数: 1345, 训练三元组个数: 483142, batch_size: 4831\n"
     ]
    }
   ],
   "source": [
    "entity_count = len(id_2_entity_name)\n",
    "relation_count = len(id_2_relation_name)\n",
    "train_count = len(train_triples)\n",
    "batch_count = 100\n",
    "batch_size = train_count//100\n",
    "print(f\"实体个数: {entity_count}, 关系个数: {relation_count}, 训练三元组个数: {train_count}, batch_size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = train_count//100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrupt_head_triple(h, t, r, count, triples):\n",
    "    \"\"\"\n",
    "    通过打乱head生成一个负样本\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        new_h = random.randrange(count)\n",
    "        # 过滤false negative的样本\n",
    "        if new_h != h and (new_h, t, r) not in triples:\n",
    "            break\n",
    "    return (new_h, t, r)\n",
    "\n",
    "def get_corrupt_tail_triple(h, t, r, count, triples):\n",
    "    \"\"\"\n",
    "    通过打乱tail生成一个负样本\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        new_t = random.randrange(count)\n",
    "        if new_t != t and (h, new_t, r) not in triples:\n",
    "            break\n",
    "    return (h, new_t, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_list(triple_list, num_batches):\n",
    "    batch_size = len(triple_list) // num_batches\n",
    "    batch_list = [0] * num_batches\n",
    "    for i in range(num_batches - 1):\n",
    "        batch_list[i] = triple_list[i * batch_size: (i + 1) * batch_size]\n",
    "    batch_list[num_batches - 1] = triple_list[(num_batches - 1) * batch_size:]\n",
    "    return batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_all(triple_list, entity_count, set_of_triples):\n",
    "    negative_triple_list = [get_corrupt_head_triple(h, t, r, entity_count, set_of_triples) if random.random() < 0.5\n",
    "                            else get_corrupt_tail_triple(h, t, r, entity_count, set_of_triples) for h, t, r in triple_list]\n",
    "    pos_h = [h for h,_,_ in triple_list]\n",
    "    pos_t = [t for _,t,_ in triple_list]\n",
    "    pos_r = [r for _,_,r in triple_list]\n",
    "    neg_h = [h for h,_,_ in negative_triple_list]\n",
    "    neg_t = [t for _,t,_ in negative_triple_list]\n",
    "    neg_r = [r for _,_,r in negative_triple_list]\n",
    "    return np.array(pos_h), np.array(pos_t), np.array(pos_r), np.array(neg_h), np.array(neg_t), np.array(neg_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_list = train_triples[300:303]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(543, 544, 118) 1992_Winter_Olympics, /olympics/olympic_games/sports, Ice_Hockey\n",
      "(545, 546, 143) Europe, /base/locations/continents/countries_within, Poland\n",
      "(547, 548, 37) Fredric_March, /award/award_nominee/award_nominations./award/award_nomination/award, Tony_Award_for_Best_Actor_in_a_Play\n"
     ]
    }
   ],
   "source": [
    "for triple in batch_list:\n",
    "    print(triple, string_format_triple(*triple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_h, p_t, p_r, n_h, n_t, n_r = get_batch_all(batch_list, entity_count, set_triples_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_h: [543 545 547]\n",
      "p_t: [544 546 548]\n",
      "p_r: [118 143  37]\n",
      "n_h: [  543 11926  9398]\n",
      "n_t: [11031   546   548]\n",
      "n_r: [118 143  37]\n"
     ]
    }
   ],
   "source": [
    "print(f\"p_h: {p_h}\")\n",
    "print(f\"p_t: {p_t}\")\n",
    "print(f\"p_r: {p_r}\")\n",
    "print(f\"n_h: {n_h}\")\n",
    "print(f\"n_t: {n_t}\")\n",
    "print(f\"n_r: {n_r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992_Winter_Olympics, /olympics/olympic_games/sports, Ice_Hockey 正样本\n",
      "Europe, /base/locations/continents/countries_within, Poland 正样本\n",
      "Fredric_March, /award/award_nominee/award_nominations./award/award_nomination/award, Tony_Award_for_Best_Actor_in_a_Play 正样本\n"
     ]
    }
   ],
   "source": [
    "for h, t, r in zip(p_h, p_t, p_r):\n",
    "    print(string_format_triple(h, t, r), \"正样本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992_Winter_Olympics, /olympics/olympic_games/sports, North_Atlantic_Treaty_Organization_(NATO) 负样本\n",
      "Russian_American, /base/locations/continents/countries_within, Poland 负样本\n",
      "Jared_Leto, /award/award_nominee/award_nominations./award/award_nomination/award, Tony_Award_for_Best_Actor_in_a_Play 负样本\n"
     ]
    }
   ],
   "source": [
    "for h, t, r in zip(n_h, n_t, n_r):\n",
    "    print(string_format_triple(h, t, r), \"负样本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_random(triple_list, batch_size, entity_count, set_of_triples):\n",
    "    batch_triple_list = random.sample(triple_list, batch_size)\n",
    "    negative_triple_list = [get_corrupt_head_triple(h, t, r, entity_count, set_of_triples) if random.random() < 0.5\n",
    "                            else get_corrupt_tail_triple(h, t, r, entity_count, set_of_triples) for h, t, r in batch_triple_list]\n",
    "    pos_h = [h for h,_,_ in batch_triple_list]\n",
    "    pos_t = [t for _,t,_ in batch_triple_list]\n",
    "    pos_r = [r for _,_,r in batch_triple_list]\n",
    "    neg_h = [h for h,_,_ in negative_triple_list]\n",
    "    neg_t = [t for _,t,_ in negative_triple_list]\n",
    "    neg_r = [r for _,_,r in negative_triple_list]\n",
    "    return np.array(pos_h), np.array(pos_t), np.array(pos_r), np.array(neg_h), np.array(neg_t), np.array(neg_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransE(nn.Module):\n",
    "    def __init__(self, entity_count, relation_count, embedding_dimension):\n",
    "        super(TransE, self).__init__()\n",
    "        # 实体embedding\n",
    "        self._ent_embeddings = nn.Embedding(entity_count, embedding_dimension)\n",
    "        # 关系embedding\n",
    "        self._rel_embeddings = nn.Embedding(relation_count, embedding_dimension)\n",
    "        # weight初始化\n",
    "        nn.init.xavier_uniform_(self._ent_embeddings.weight.data)\n",
    "        nn.init.xavier_uniform_(self._ent_embeddings.weight.data)\n",
    "        \n",
    "    def _scoring_function(self, h, t, r):\n",
    "        \"\"\"\n",
    "        fr(h, t, r)\n",
    "        \"\"\"\n",
    "        # 按元素求绝对值\n",
    "        s = -torch.abs(h + r - t) # L1 distance\n",
    "        # s = -((h + r - t) ** 2) # L2 distance\n",
    "        s = torch.sum(s, 1)\n",
    "        return s\n",
    "    \n",
    "    def _norm_regularization_loss(self, embeddings, dim=1):\n",
    "        \"\"\"\n",
    "        L2 regularization\n",
    "        \"\"\"\n",
    "        norm = torch.sum(embeddings ** 2, dim=dim, keepdim=True)\n",
    "        return torch.sum(torch.max(norm - Variable(torch.FloatTensor([1.0])), Variable(torch.FloatTensor([0.0]))))\n",
    "        \n",
    "    def forward(self, pos_batch_h, pos_batch_t, pos_batch_r, neg_batch_h, neg_batch_t, neg_batch_r):\n",
    "        \"\"\"\n",
    "        示例三元组：(1,5,8), (2,6,9), (3,7,10)\n",
    "        :param pos_batch_h: np.array([1,2,3]) 正样本对应的h实体列表\n",
    "        :param pos_batch_t: np.array([5,6,7]) 正样本对应的t实体列表\n",
    "        :param pos_batch_r: np.array([8,9,10]) 正样本对应的r关系列表\n",
    "        :param neg_batch_h: 负样本。。。\n",
    "        :param neg_batch_t:\n",
    "        :param neg_batch_r:\n",
    "        \"\"\"\n",
    "        # pos_, p_ => 正样本, neg_, n_ => 负样本\n",
    "        pos_h = Variable(torch.from_numpy(pos_batch_h))\n",
    "        pos_t = Variable(torch.from_numpy(pos_batch_t))\n",
    "        pos_r = Variable(torch.from_numpy(pos_batch_r))\n",
    "        neg_h = Variable(torch.from_numpy(neg_batch_h))\n",
    "        neg_t = Variable(torch.from_numpy(neg_batch_t))\n",
    "        neg_r = Variable(torch.from_numpy(neg_batch_r))\n",
    "        p_h = self._ent_embeddings(pos_h)\n",
    "        p_t = self._ent_embeddings(pos_t)\n",
    "        p_r = self._rel_embeddings(pos_r)\n",
    "        n_h = self._ent_embeddings(neg_h)\n",
    "        n_t = self._ent_embeddings(neg_t)\n",
    "        n_r = self._rel_embeddings(neg_r)\n",
    "        p_score = self._scoring_function(p_h, p_t, p_r)\n",
    "        n_score = self._scoring_function(n_h, n_t, n_r)\n",
    "        # https://pytorch.org/docs/stable/nn.html#torch.nn.MarginRankingLoss\n",
    "        criterion = nn.MarginRankingLoss(1, False) # False 意味一个batch中的多个样本loss求和\n",
    "        y = Variable(torch.Tensor([1])) # y为1，正样本对应分数更高\n",
    "        loss = criterion(p_score, n_score, y)\n",
    "        # 下面这个是给loss加上L2 reg term\n",
    "        # ent_embeddings = self._ent_embeddings(torch.cat([pos_h, pos_t, neg_h, neg_t]))\n",
    "        # rel_embeddings = self._rel_embeddings(torch.cat([pos_r, neg_r]))\n",
    "        # loss = loss + self._norm_regularization_loss(ent_embeddings) + self._norm_regularization_loss(rel_embeddings)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, h, t, r):\n",
    "        \"\"\"\n",
    "        注意，得出的fr(h, t, r) score\n",
    "        \"\"\"\n",
    "        p_h = self._ent_embeddings(Variable(torch.from_numpy(h)))\n",
    "        p_t = self._ent_embeddings(Variable(torch.from_numpy(t)))\n",
    "        p_r = self._rel_embeddings(Variable(torch.from_numpy(r)))\n",
    "        return self._scoring_function(p_h, p_t, p_r)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model loss demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1273,  0.2170,  0.7699,  0.6952,  0.1699],\n",
       "        [-0.6160,  0.4615,  0.3355,  0.6024,  0.5683],\n",
       "        [ 0.3537, -0.4161,  0.8021, -0.3998, -0.8053],\n",
       "        [-0.0591,  0.1567,  0.7029, -0.8015,  0.4734]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_entity_count = 3\n",
    "demo_relation_count = 4\n",
    "demo_embedding_dimension = 5\n",
    "demo_ent_embeddings = nn.Embedding(demo_entity_count, demo_embedding_dimension)\n",
    "demo_rel_embeddings = nn.Embedding(demo_relation_count, demo_embedding_dimension)\n",
    "nn.init.xavier_uniform_(demo_ent_embeddings.weight.data)\n",
    "nn.init.xavier_uniform_(demo_rel_embeddings.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1263, -0.5359, -0.6048,  0.0355, -0.4541],\n",
       "        [ 0.1900, -0.3014,  0.1206,  0.6039,  0.7024],\n",
       "        [ 0.6603, -0.3395, -0.7639,  0.8320,  0.7146]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ent_embeddings.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ent_embeddings.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1273,  0.2170,  0.7699,  0.6952,  0.1699],\n",
       "        [-0.6160,  0.4615,  0.3355,  0.6024,  0.5683],\n",
       "        [ 0.3537, -0.4161,  0.8021, -0.3998, -0.8053],\n",
       "        [-0.0591,  0.1567,  0.7029, -0.8015,  0.4734]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_rel_embeddings.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_rel_embeddings.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_pos_batch_h = np.array([1,2])\n",
    "demo_pos_batch_t = np.array([0,1])\n",
    "demo_pos_batch_r = np.array([3,2])\n",
    "demo_neg_batch_h = np.array([1,2])\n",
    "demo_neg_batch_t = np.array([2,0])\n",
    "demo_neg_batch_r = np.array([3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_pos_h = Variable(torch.from_numpy(demo_pos_batch_h))\n",
    "demo_pos_t = Variable(torch.from_numpy(demo_pos_batch_t))\n",
    "demo_pos_r = Variable(torch.from_numpy(demo_pos_batch_r))\n",
    "demo_neg_h = Variable(torch.from_numpy(demo_neg_batch_h))\n",
    "demo_neg_t = Variable(torch.from_numpy(demo_neg_batch_t))\n",
    "demo_neg_r = Variable(torch.from_numpy(demo_neg_batch_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_p_h = demo_ent_embeddings(demo_pos_h)\n",
    "demo_p_t = demo_ent_embeddings(demo_pos_t)\n",
    "demo_p_r = demo_rel_embeddings(demo_pos_r)\n",
    "demo_n_h = demo_ent_embeddings(demo_neg_h)\n",
    "demo_n_t = demo_ent_embeddings(demo_neg_t)\n",
    "demo_n_r = demo_rel_embeddings(demo_neg_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo_p_h: tensor([[ 0.1900, -0.3014,  0.1206,  0.6039,  0.7024],\n",
      "        [ 0.6603, -0.3395, -0.7639,  0.8320,  0.7146]])\n",
      "demo_p_t: tensor([[ 0.1263, -0.5359, -0.6048,  0.0355, -0.4541],\n",
      "        [ 0.1900, -0.3014,  0.1206,  0.6039,  0.7024]])\n",
      "demo_p_r: tensor([[-0.0591,  0.1567,  0.7029, -0.8015,  0.4734],\n",
      "        [ 0.3537, -0.4161,  0.8021, -0.3998, -0.8053]])\n",
      "demo_n_h: tensor([[ 0.1900, -0.3014,  0.1206,  0.6039,  0.7024],\n",
      "        [ 0.6603, -0.3395, -0.7639,  0.8320,  0.7146]])\n",
      "demo_n_t: tensor([[ 0.6603, -0.3395, -0.7639,  0.8320,  0.7146],\n",
      "        [ 0.1263, -0.5359, -0.6048,  0.0355, -0.4541]])\n",
      "demo_n_r: tensor([[-0.0591,  0.1567,  0.7029, -0.8015,  0.4734],\n",
      "        [ 0.3537, -0.4161,  0.8021, -0.3998, -0.8053]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"demo_p_h: {demo_p_h}\")\n",
    "print(f\"demo_p_t: {demo_p_t}\")\n",
    "print(f\"demo_p_r: {demo_p_r}\")\n",
    "print(f\"demo_n_h: {demo_n_h}\")\n",
    "print(f\"demo_n_t: {demo_n_t}\")\n",
    "print(f\"demo_n_r: {demo_n_r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_p_score = -torch.abs(demo_p_h + demo_p_r - demo_p_t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0046, -0.3913, -1.4283, -0.2332, -1.6299],\n",
       "        [-0.8240, -0.4541, -0.0824, -0.1716, -0.7931]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_p_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_p_score = torch.sum(demo_p_score, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.6872, -2.3253])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_p_score\n",
    "# 两个样本，之后通过margin loss function求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_n_score = -torch.abs(demo_n_h + demo_n_r - demo_n_t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5295, -0.1948, -1.5874, -1.0296, -0.4612],\n",
       "        [-0.8877, -0.2196, -0.6430, -0.3967, -0.3634]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_n_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_n_score = torch.sum(demo_n_score, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.8026, -2.5104])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_n_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MarginRankingLoss(1, False)\n",
    "y = Variable(torch.Tensor([1])) \n",
    "loss = criterion(demo_p_score, demo_n_score, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6996)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面演示OpenKE实现的等价性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "_openke_p_score = torch.abs(demo_p_h + demo_p_r - demo_p_t)\n",
    "_openke_n_score = torch.abs(demo_n_h + demo_n_r - demo_n_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0046,  0.3913,  1.4283,  0.2332,  1.6299],\n",
       "        [ 0.8240,  0.4541,  0.0824,  0.1716,  0.7931]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_openke_p_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5295,  0.1948,  1.5874,  1.0296,  0.4612],\n",
       "        [ 0.8877,  0.2196,  0.6430,  0.3967,  0.3634]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_openke_n_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "_openke_p_score = _openke_p_score.view(-1, 1, demo_embedding_dimension)\n",
    "_openke_n_score = _openke_n_score.view(-1, 1, demo_embedding_dimension)\n",
    "_openke_p_score = torch.sum(torch.mean(_openke_p_score, 1), 1)\n",
    "_openke_n_score = torch.sum(torch.mean(_openke_n_score, 1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.6872,  2.3253])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_openke_p_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.8026,  2.5104])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_openke_n_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MarginRankingLoss(1, False)\n",
    "y = Variable(torch.Tensor([-1]))\n",
    "openke_loss = criterion(_openke_p_score, _openke_n_score, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6996)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openke_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransE(entity_count, relation_count, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.RMSprop(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_list = get_batch_list(train_triples, batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_epoch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "for epoch in tqdm(range(number_of_epoch)):\n",
    "    total_loss = 0.0\n",
    "    random.shuffle(train_batch_list)\n",
    "    for batch_triples in train_batch_list:\n",
    "        p_h, p_t, p_r, n_h, n_t, n_r = get_batch_all(batch_triples, entity_count, set_triples_train)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # 前向\n",
    "        loss = model(p_h, p_t, p_r, n_h, n_t, n_r)\n",
    "        total_loss = total_loss + loss.item()\n",
    "        # 求梯度\n",
    "        loss.backward()\n",
    "        # 梯度下降\n",
    "        optimizer.step()\n",
    "    print(f\"{epoch}, train {total_loss}\")\n",
    "    train_losses.append(total_loss)\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        p_h, p_t, p_r, n_h, n_t, n_r = get_batch_random(valid_triples, batch_size, entity_count, set_triples_train_valid)\n",
    "        loss = model(p_h, p_t, p_r, n_h, n_t, n_r)\n",
    "        print(f\"{epoch}, valid {loss.item()}\")\n",
    "        valid_losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"transe_fb15k.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransE(entity_count, relation_count, 100)\n",
    "model.load_state_dict(torch.load(\"res/transe_fb15k.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test link prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_answer_for_head(t, r, count):\n",
    "    p_h = np.arange(count)\n",
    "    p_t = np.full(count, t)\n",
    "    p_r = np.full(count, r)\n",
    "    return np.array(p_h), np.array(p_t), np.array(p_r)\n",
    "\n",
    "def get_candidate_answer_for_tail(h, r, count):\n",
    "    p_h = np.full(count, h)\n",
    "    p_t = np.arange(count)\n",
    "    p_r = np.full(count, r)\n",
    "    return np.array(p_h), np.array(p_t), np.array(p_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2]))"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_candidate_answer_for_head(1, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 1, 1, 1]), array([0, 1, 2, 3, 4]), array([2, 2, 2, 2, 2]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_candidate_answer_for_tail(1, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_tot = 0 # head命中前10\n",
    "l_filter_tot = 0 # 过滤已知三元组之后head命中前10\n",
    "l3_tot = 0 # head命中前3\n",
    "l3_filter_tot = 0 # 过滤已知三元组之后head命中前3\n",
    "l1_tot = 0 # head命中前1\n",
    "l1_filter_tot = 0 # 过滤已知三元组之后head命中前1\n",
    "\n",
    "l_rank = 0 # head命中的rank之和，为了计算MR\n",
    "l_filter_rank = 0 # 过滤已知三元组之后head命中的rank之和\n",
    "l_reci_rank = 0 # head命中的rank倒数之和，为了计算MRR\n",
    "l_filter_reci_rank = 0 # 过滤已知三元组之后head命中的rank倒数之和\n",
    "\n",
    "# 以下变量用于tail指标计算\n",
    "r_tot = 0\n",
    "r_filter_tot = 0\n",
    "r3_tot = 0\n",
    "r3_filter_tot = 0\n",
    "r1_tot = 0\n",
    "r1_filter_tot = 0\n",
    "\n",
    "r_rank = 0\n",
    "r_filter_rank = 0\n",
    "r_reci_rank = 0\n",
    "r_filter_reci_rank = 0\n",
    "\n",
    "for test_triple in tqdm(test_triples):\n",
    "    h, t, r = test_triple\n",
    "    \n",
    "    p_h, p_t, p_r = get_candidate_answer_for_head(t, r, entity_count)\n",
    "    scores = model.predict(p_h, p_t, p_r)\n",
    "    \n",
    "    triple_score = scores[h]\n",
    "    l_s = 0\n",
    "    l_filter_s = 0\n",
    "    for entity_id in range(entity_count):\n",
    "        if entity_id != h:\n",
    "            score = scores[entity_id]\n",
    "            if score > triple_score:\n",
    "                l_s += 1\n",
    "                if (entity_id, t, r) not in set_triples_train_valid:\n",
    "                    l_filter_s += 1\n",
    "    if l_s < 10:\n",
    "        l_tot += 1\n",
    "    if l_filter_s < 10:\n",
    "        l_filter_tot += 1\n",
    "    if l_s < 3:\n",
    "        l3_tot += 1\n",
    "    if l_filter_s < 3:\n",
    "        l3_filter_tot += 1\n",
    "    if l_s < 1:\n",
    "        l1_tot += 1\n",
    "    if l_filter_s < 1:\n",
    "        l1_filter_tot += 1\n",
    "    \n",
    "    l_rank += (1+l_s)  # 1+l_s即当前head的rank\n",
    "    l_filter_rank += (1+l_filter_s)\n",
    "    l_reci_rank += 1/(1+l_s)\n",
    "    l_filter_reci_rank += 1/(1+l_filter_s)\n",
    "    \n",
    "    p_h, p_t, p_r = get_candidate_answer_for_tail(h, r, entity_count)\n",
    "    scores = model.predict(p_h, p_t, p_r)\n",
    "    \n",
    "    triple_score = scores[t]\n",
    "    r_s = 0\n",
    "    r_filter_s = 0\n",
    "    for entity_id in range(entity_count):\n",
    "        if entity_id != t:\n",
    "            score = scores[entity_id]\n",
    "            if score > triple_score:\n",
    "                r_s += 1\n",
    "                if (h, entity_id, r) not in set_triples_train_valid:\n",
    "                    r_filter_s += 1\n",
    "    if r_s < 10:\n",
    "        r_tot += 1\n",
    "    if r_filter_s < 10:\n",
    "        r_filter_tot += 1\n",
    "    if r_s < 3:\n",
    "        r3_tot += 1\n",
    "    if r_filter_s < 3:\n",
    "        r3_filter_tot += 1\n",
    "    if r_s < 1:\n",
    "        r1_tot += 1\n",
    "    if r_filter_s < 1:\n",
    "        r1_filter_tot += 1\n",
    "        \n",
    "    r_rank += (1+r_s)\n",
    "    r_filter_rank += (1+r_filter_s)\n",
    "    r_reci_rank += 1/(1+r_s)\n",
    "    r_filter_reci_rank += 1/(1+r_filter_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_mr = l_rank / test_count\n",
    "r_mr = r_rank / test_count\n",
    "l_mrr = l_reci_rank / test_count\n",
    "r_mrr = r_reci_rank / test_count\n",
    "lp_hit10 = l_tot / test_count\n",
    "rp_hit10 = r_tot / test_count\n",
    "lp_hit3 = l3_tot / test_count\n",
    "rp_hit3 = r3_tot / test_count\n",
    "lp_hit1 = l1_tot / test_count\n",
    "rp_hit1 = r1_tot / test_count\n",
    "\n",
    "filter_l_mr = l_filter_rank / test_count\n",
    "filter_r_mr = r_filter_rank / test_count\n",
    "filter_l_mrr = l_filter_reci_rank / test_count\n",
    "filter_r_mrr = r_filter_reci_rank / test_count\n",
    "filter_lp_hit10 = l_filter_tot / test_count\n",
    "filter_rp_hit10 = r_filter_tot / test_count\n",
    "filter_lp_hit3 = l3_filter_tot / test_count\n",
    "filter_rp_hit3 = r3_filter_tot / test_count\n",
    "filter_lp_hit1 = l1_filter_tot / test_count\n",
    "filter_rp_hit1 = r1_filter_tot / test_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    \"metric\": [\"predict head\", \"predict tail\", \"average\", \"predict head(filter)\", \"predict tail(filter)\", \"average(filter)\"],\n",
    "    \"MRR\": [l_mrr, r_mrr, (l_mrr+r_mrr)/2, filter_l_mrr, filter_r_mrr, (filter_l_mrr+filter_r_mrr)/2],\n",
    "    \"MR\": [l_mr, r_mr, (l_mr+r_mr)/2, filter_l_mr, filter_r_mr, (filter_l_mr+filter_r_mr)/2],\n",
    "    \"P@10\": [lp_hit10, rp_hit10, (lp_hit10+rp_hit10)/2, filter_lp_hit10, filter_rp_hit10, (filter_lp_hit10+filter_rp_hit10)/2],\n",
    "    \"P@3\": [lp_hit3, rp_hit3, (lp_hit3+rp_hit3)/2, filter_lp_hit3, filter_rp_hit3, (filter_lp_hit3+filter_rp_hit3)/2],\n",
    "    \"P@1\": [lp_hit1, rp_hit1, (lp_hit1+rp_hit1)/2, filter_lp_hit1, filter_rp_hit1, (filter_lp_hit1+filter_rp_hit1)/2]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def predict_head_entity(t, r, k):\n",
    "    \"\"\"\n",
    "    :param t: 尾实体id\n",
    "    :param r: 关系id\n",
    "    :param k: top k\n",
    "    \"\"\"\n",
    "    test_h = np.array(range(entity_count))\n",
    "    test_r = np.array([r] * entity_count)\n",
    "    test_t = np.array([t] * entity_count)\n",
    "    scores = model.predict(test_h, test_t, test_r)\n",
    "    result = scores.data.numpy().reshape(-1).argsort()[-k:]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entity_id_by_entity_name(n):\n",
    "    for e_id, e_name in id_2_entity_name.items():\n",
    "        if e_name == n:\n",
    "            return e_id\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relation_id_by_relation_name(r):\n",
    "    for r_id, r_name in id_2_relation_name.items():\n",
    "        if r_name == r:\n",
    "            return r_id\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = find_entity_id_by_entity_name(\"China\")\n",
    "r = find_relation_id_by_relation_name(\"/people/person/nationality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3381, 84)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = predict_head_entity(t, r, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1622,  3024, 12023,  2332,  9905,  2722, 10393,  9836, 11838,\n",
       "        9136])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['William_Chang',\n",
       " 'Zhang_Ziyi',\n",
       " 'Eric_Tsang',\n",
       " 'Joan_Chen',\n",
       " 'Chow_Yun-Fat',\n",
       " 'Andy_Lau',\n",
       " 'Daniel_Wu',\n",
       " 'Jet_Li',\n",
       " 'Tony_Leung_Chiu_Wai',\n",
       " 'Sammo_Hung']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id_2_entity_name[h] for h in hs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, True, True, False, True, True, True]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(h,t,r) in set_triples_train_valid_test for h in hs] # False表示在训练集，验证集和测试集中不存在的三元组"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test triple classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_h, p_t, p_r, n_h, n_t, n_r = get_batch_all(valid_triples, entity_count, set_triples_train_valid)\n",
    "p_scores = model.predict(p_h, p_t, p_r)\n",
    "n_scores = model.predict(n_h, n_t, n_r)\n",
    "p_scores = p_scores.cpu().data.numpy() # tensor to numpy\n",
    "n_scores = n_scores.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# 关系id => 验证样本下标列表，之后可以由此p_scores和n_scores中找到对应该id的所有score\n",
    "relation_triple_indexes = defaultdict(list)\n",
    "for i, triple in enumerate(valid_triples):\n",
    "    _, _, r = triple\n",
    "    relation_triple_indexes[r].append(i)\n",
    "relation_triple_indexes = dict(relation_triple_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_thresholds(relation_triple_indexes, relation_count, p_scores, n_scores):\n",
    "    try_interval = 0.01\n",
    "    best_thresholds_for_relation = np.zeros(relation_count)\n",
    "    for relation_id in range(relation_count):\n",
    "        if relation_id not in relation_triple_indexes:\n",
    "            continue\n",
    "        relation_p_scores = [p_scores[i] for i in relation_triple_indexes[relation_id]]\n",
    "        relation_n_scores = [n_scores[i] for i in relation_triple_indexes[relation_id]]\n",
    "        relation_all_scores = relation_p_scores + relation_n_scores\n",
    "        max_score = max(relation_all_scores)\n",
    "        min_score = min(relation_all_scores)\n",
    "        \n",
    "        # 根据这个关系id对应的score范围，以0.01为粒度，尝试找到准确率最高的阈值\n",
    "        best_correct_count = 0\n",
    "        best_threshold = 0\n",
    "        for threshold in np.arange(min_score, max_score, try_interval):\n",
    "            correct_count = 0\n",
    "            for i in relation_triple_indexes[relation_id]:\n",
    "                if p_scores[i] >= threshold:\n",
    "                    correct_count += 1\n",
    "                if n_scores[i] < threshold:\n",
    "                    correct_count += 1\n",
    "            if correct_count > best_correct_count:\n",
    "                best_threshold = threshold\n",
    "                best_correct_count = correct_count\n",
    "        best_thresholds_for_relation[relation_id] = best_threshold\n",
    "    return best_thresholds_for_relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thresholds_for_relation = get_best_thresholds(relation_triple_indexes, relation_count, p_scores, n_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_h, p_t, p_r, n_h, n_t, n_r = get_batch_all(test_triples, entity_count, set_triples_train_valid_test)\n",
    "p_scores = model.predict(p_h, p_t, p_r)\n",
    "n_scores = model.predict(n_h, n_t, n_r)\n",
    "p_scores = p_scores.cpu().data.numpy()\n",
    "n_scores = n_scores.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triple classification accuracy: 0.9643197036130655\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "total_count = 0\n",
    "for i,triple in enumerate(test_triples):\n",
    "    _,_,r = triple\n",
    "    # 如果valid中没有对应r的样本，则无法测试...\n",
    "    if r not in relation_triple_indexes:\n",
    "        continue\n",
    "    threshold = best_thresholds_for_relation[r]\n",
    "    total_count += 2 # 因为实际上还有一倍的负样本\n",
    "    if p_scores[i] >= threshold:\n",
    "        correct_count += 1\n",
    "    if n_scores[i] < threshold:\n",
    "        correct_count += 1\n",
    "        \n",
    "acc = correct_count / total_count\n",
    "print(f\"triple classification accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_triple(h, t, r):\n",
    "    score = model.predict(np.array([h]), np.array([t]), np.array([r])).data.numpy()\n",
    "    best_threshold = best_thresholds_for_relation[r]\n",
    "    if score > best_threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = find_entity_id_by_entity_name(\"China\")\n",
    "t = find_entity_id_by_entity_name(\"Tsinghua_University\")\n",
    "t = find_entity_id_by_entity_name(\"University_of_Tokyo\")\n",
    "t = find_entity_id_by_entity_name(\"University_of_North_Texas\")\n",
    "r = find_relation_id_by_relation_name(\"/location/location/contains\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'China, /location/location/contains, University_of_North_Texas'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_format_triple(h, t, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(h, t, r) in set_triples_train_valid_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_triple(h, t, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
